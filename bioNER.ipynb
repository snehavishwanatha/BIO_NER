{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    \n",
    "    tweet_tokens = []\n",
    "    tweet_tags = []\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            if tweet_tokens:\n",
    "                tokens.append(tweet_tokens)\n",
    "                tags.append(tweet_tags)\n",
    "            tweet_tokens = []\n",
    "            tweet_tags = []\n",
    "        else:\n",
    "            token, tag = line.split()\n",
    "            tweet_tokens.append(token)\n",
    "            tweet_tags.append(tag)\n",
    "            \n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('/home/sneha/Documents/bioNER/bioNER.csv', header = None)\n",
    "df[0].replace('  ', np.nan, inplace=True)\n",
    "df= df.dropna(subset=[0])\n",
    "train_tokens, train_tags = df[0], df[1]\n",
    "df = pd.read_csv('/home/sneha/Documents/bioNER/bioTEST.csv', header = None)\n",
    "df[0].replace('  ', np.nan, inplace=True)\n",
    "df= df.dropna(subset=[0])\n",
    "test_tokens, test_tags = df[0], df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fr</td>\n",
       "      <td>Outside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MyKembangSepatu</td>\n",
       "      <td>Outside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Common</td>\n",
       "      <td>Outside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asthma</td>\n",
       "      <td>B_Disease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>steroids</td>\n",
       "      <td>B_Drug</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0          1\n",
       "0               Fr    Outside\n",
       "1  MyKembangSepatu    Outside\n",
       "2           Common    Outside\n",
       "3           asthma  B_Disease\n",
       "4         steroids     B_Drug"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0                 App\n",
       " 1         facilitates\n",
       " 2               early\n",
       " 3           detection\n",
       " 4           treatment\n",
       " 5                  of\n",
       " 6                COPD\n",
       " 7        exacerbation\n",
       " 8            symptoms\n",
       " 9                   A\n",
       " 10            digital\n",
       " 11             health\n",
       " 12        application\n",
       " 13                for\n",
       " 14                rep\n",
       " 15               http\n",
       " 16                  t\n",
       " 17                 co\n",
       " 18         xoHWYCh0Ty\n",
       " 19             Saving\n",
       " 20              lives\n",
       " 21           creating\n",
       " 22         difference\n",
       " 23               thru\n",
       " 24           critical\n",
       " 25               care\n",
       " 26                  A\n",
       " 27               case\n",
       " 28                 of\n",
       " 29               COPD\n",
       "              ...     \n",
       " 23831          health\n",
       " 23832         hazards\n",
       " 23833              of\n",
       " 23834          global\n",
       " 23835         warming\n",
       " 23836            http\n",
       " 23837               t\n",
       " 23838              co\n",
       " 23839      UxoKG0dXyB\n",
       " 23840          asthma\n",
       " 23842               A\n",
       " 23843        Milpitas\n",
       " 23844               8\n",
       " 23845            year\n",
       " 23846             old\n",
       " 23847            died\n",
       " 23848        suddenly\n",
       " 23849              on\n",
       " 23850             Dec\n",
       " 23851              27\n",
       " 23852           after\n",
       " 23853       suffering\n",
       " 23854              an\n",
       " 23855          asthma\n",
       " 23856          attack\n",
       " 23857            http\n",
       " 23858               t\n",
       " 23859              co\n",
       " 23860      H70UTtdAga\n",
       " 23861          asthma\n",
       " Name: 0, Length: 22440, dtype: object, 0                         Outside\n",
       " 1                         Outside\n",
       " 2                         Outside\n",
       " 3                         Outside\n",
       " 4                         Outside\n",
       " 5                         Outside\n",
       " 6                       B_Disease\n",
       " 7        B_Symptom-or-Side-Effect\n",
       " 8                         Outside\n",
       " 9                         Outside\n",
       " 10                        Outside\n",
       " 11                        Outside\n",
       " 12                        Outside\n",
       " 13                        Outside\n",
       " 14                        Outside\n",
       " 15                        Outside\n",
       " 16                        Outside\n",
       " 17                        Outside\n",
       " 18                        Outside\n",
       " 19                        Outside\n",
       " 20                        Outside\n",
       " 21                        Outside\n",
       " 22                        Outside\n",
       " 23                        Outside\n",
       " 24                        Outside\n",
       " 25                        Outside\n",
       " 26                        Outside\n",
       " 27                        Outside\n",
       " 28                        Outside\n",
       " 29                      B_Disease\n",
       "                    ...           \n",
       " 23831                     Outside\n",
       " 23832                     Outside\n",
       " 23833                     Outside\n",
       " 23834                     Outside\n",
       " 23835                     Outside\n",
       " 23836                     Outside\n",
       " 23837                     Outside\n",
       " 23838                     Outside\n",
       " 23839                     Outside\n",
       " 23840                     Outside\n",
       " 23842                     Outside\n",
       " 23843                     Outside\n",
       " 23844                     Outside\n",
       " 23845                     Outside\n",
       " 23846                     Outside\n",
       " 23847                     Outside\n",
       " 23848                     Outside\n",
       " 23849                     Outside\n",
       " 23850                     Outside\n",
       " 23851                     Outside\n",
       " 23852                     Outside\n",
       " 23853                     Outside\n",
       " 23854                     Outside\n",
       " 23855                   B_Disease\n",
       " 23856                     Outside\n",
       " 23857                     Outside\n",
       " 23858                     Outside\n",
       " 23859                     Outside\n",
       " 23860                     Outside\n",
       " 23861                   B_Disease\n",
       " Name: 1, Length: 22440, dtype: object)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_tokens, train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0                     Fr\n",
       " 1        MyKembangSepatu\n",
       " 2                 Common\n",
       " 3                 asthma\n",
       " 4               steroids\n",
       " 5                 linked\n",
       " 6                     to\n",
       " 7                   side\n",
       " 8                effects\n",
       " 9                     in\n",
       " 10               adrenal\n",
       " 11                glands\n",
       " 12                 After\n",
       " 13              stopping\n",
       " 14              steroids\n",
       " 15                     c\n",
       " 16                  http\n",
       " 17                     t\n",
       " 18                    co\n",
       " 19            QmBrVhEzkm\n",
       " 21             Nutrition\n",
       " 22             treatment\n",
       " 23                    in\n",
       " 24          hospitalized\n",
       " 25              patients\n",
       " 26                  with\n",
       " 27                  COPD\n",
       " 28              improved\n",
       " 29                health\n",
       " 30              outcomes\n",
       "               ...       \n",
       " 10972                for\n",
       " 10973            chronic\n",
       " 10974              cough\n",
       " 10975            related\n",
       " 10976          laryngeal\n",
       " 10977         sensations\n",
       " 10978               http\n",
       " 10979                  t\n",
       " 10980                 co\n",
       " 10981         M9tNRdIT4h\n",
       " 10983                 RT\n",
       " 10984       WorldLungFdn\n",
       " 10985        Fascinating\n",
       " 10986                How\n",
       " 10987                  a\n",
       " 10988              small\n",
       " 10989              group\n",
       " 10990                 of\n",
       " 10991         determined\n",
       " 10992              women\n",
       " 10993            created\n",
       " 10994                  a\n",
       " 10995          smokefree\n",
       " 10996          community\n",
       " 10997                 in\n",
       " 10998              China\n",
       " 10999               http\n",
       " 11000                  t\n",
       " 11001                 co\n",
       " 11002         cZTNxE64Zx\n",
       " Name: 0, Length: 10346, dtype: object, 0          Outside\n",
       " 1          Outside\n",
       " 2          Outside\n",
       " 3        B_Disease\n",
       " 4           B_Drug\n",
       " 5          Outside\n",
       " 6          Outside\n",
       " 7          Outside\n",
       " 8          Outside\n",
       " 9          Outside\n",
       " 10         Outside\n",
       " 11         Outside\n",
       " 12         Outside\n",
       " 13         Outside\n",
       " 14          B_Drug\n",
       " 15         Outside\n",
       " 16         Outside\n",
       " 17         Outside\n",
       " 18         Outside\n",
       " 19         Outside\n",
       " 21         Outside\n",
       " 22         Outside\n",
       " 23         Outside\n",
       " 24         Outside\n",
       " 25         Outside\n",
       " 26         Outside\n",
       " 27       B_Disease\n",
       " 28         Outside\n",
       " 29         Outside\n",
       " 30         Outside\n",
       "            ...    \n",
       " 10972      Outside\n",
       " 10973    B_Disease\n",
       " 10974      Outside\n",
       " 10975      Outside\n",
       " 10976    I_Disease\n",
       " 10977    I_Disease\n",
       " 10978      Outside\n",
       " 10979      Outside\n",
       " 10980      Outside\n",
       " 10981      Outside\n",
       " 10983      Outside\n",
       " 10984      Outside\n",
       " 10985      Outside\n",
       " 10986      Outside\n",
       " 10987      Outside\n",
       " 10988      Outside\n",
       " 10989      Outside\n",
       " 10990      Outside\n",
       " 10991      Outside\n",
       " 10992      Outside\n",
       " 10993      Outside\n",
       " 10994      Outside\n",
       " 10995      Outside\n",
       " 10996      Outside\n",
       " 10997      Outside\n",
       " 10998      Outside\n",
       " 10999      Outside\n",
       " 11000      Outside\n",
       " 11001      Outside\n",
       " 11002      Outside\n",
       " Name: 1, Length: 10346, dtype: object)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokens, test_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(tokens_or_tags, special_tokens):\n",
    "    \"\"\"\n",
    "        tokens_or_tags: a list of lists of tokens or tags\n",
    "        special_tokens: some special tokens\n",
    "    \"\"\"\n",
    "    # Create a dictionary with default value 0\n",
    "    tok2idx = defaultdict(lambda: 0)\n",
    "    idx2tok = []\n",
    "    \n",
    "    # Create mappings from tokens to indices and vice versa\n",
    "    # Add special tokens to dictionaries\n",
    "    # The first special token must have index 0\n",
    "    \n",
    "    \n",
    "    idx = 0\n",
    "    for token in special_tokens:\n",
    "        idx2tok.append(token)\n",
    "        tok2idx[token] = idx\n",
    "        idx += 1\n",
    "    \n",
    "    for token in tokens_or_tags:\n",
    "        if token not in tok2idx:\n",
    "            idx2tok.append(token)\n",
    "            tok2idx[token] = idx\n",
    "            idx += 1\n",
    "    \n",
    "    return tok2idx, idx2tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = ['<UNK>', '<PAD>']\n",
    "special_tags = ['O']\n",
    "\n",
    "# Create dictionaries \n",
    "token2idx, idx2token = build_dict(train_tokens, special_tokens)\n",
    "tag2idx, idx2tag = build_dict(train_tags, special_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function build_dict.<locals>.<lambda> at 0x7f88d948fa60>, {'O': 0, 'Outside': 1, 'B_Disease': 2, 'B_Symptom-or-Side-Effect': 3, 'I_Disease': 4, 'I_Symptom-or-Side-Effect': 5, 'B_Drug': 6, 'I_Drug': 7})\n"
     ]
    }
   ],
   "source": [
    "print(tag2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next additional functions will help you to create the mapping between tokens and ids for a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'Outside', 'B_Disease', 'B_Symptom-or-Side-Effect', 'I_Disease', 'I_Symptom-or-Side-Effect', 'B_Drug', 'I_Drug']\n"
     ]
    }
   ],
   "source": [
    "print(idx2tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(len(idx2tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2idxs(tokens_list):\n",
    "    return [token2idx[word] for word in tokens_list]\n",
    "\n",
    "def tags2idxs(tags_list):\n",
    "    return [tag2idx[tag] for tag in tags_list]\n",
    "\n",
    "def idxs2words(idxs):\n",
    "    return [idx2token[idx] for idx in idxs]\n",
    "\n",
    "def idxs2tags(idxs):\n",
    "    return [idx2tag[idx] for idx in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 11,\n",
       " 28,\n",
       " 7,\n",
       " 8,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 29,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 7,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 41,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 7,\n",
       " 52,\n",
       " 34,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 52,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 51,\n",
       " 7,\n",
       " 34,\n",
       " 67,\n",
       " 68,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 64,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 34,\n",
       " 51,\n",
       " 67,\n",
       " 68,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 34,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 41,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 8,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 88,\n",
       " 89,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 8,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 112,\n",
       " 113,\n",
       " 123,\n",
       " 124,\n",
       " 7,\n",
       " 34,\n",
       " 41,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 34,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 34,\n",
       " 152,\n",
       " 153,\n",
       " 51,\n",
       " 7,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 158,\n",
       " 159,\n",
       " 76,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 34,\n",
       " 168,\n",
       " 169,\n",
       " 41,\n",
       " 170,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 171,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 41,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 7,\n",
       " 52,\n",
       " 34,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 166,\n",
       " 177,\n",
       " 178,\n",
       " 10,\n",
       " 7,\n",
       " 30,\n",
       " 34,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 179,\n",
       " 180,\n",
       " 34,\n",
       " 6,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 41,\n",
       " 186,\n",
       " 113,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 190,\n",
       " 34,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 76,\n",
       " 196,\n",
       " 7,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 34,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 34,\n",
       " 41,\n",
       " 217,\n",
       " 126,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 218,\n",
       " 219,\n",
       " 27,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 34,\n",
       " 225,\n",
       " 226,\n",
       " 76,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 34,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 109,\n",
       " 254,\n",
       " 29,\n",
       " 255,\n",
       " 105,\n",
       " 256,\n",
       " 41,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 18,\n",
       " 19,\n",
       " 260,\n",
       " 261,\n",
       " 68,\n",
       " 262,\n",
       " 42,\n",
       " 109,\n",
       " 263,\n",
       " 264,\n",
       " 166,\n",
       " 178,\n",
       " 265,\n",
       " 7,\n",
       " 34,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 76,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 193,\n",
       " 152,\n",
       " 113,\n",
       " 272,\n",
       " 273,\n",
       " 193,\n",
       " 152,\n",
       " 274,\n",
       " 34,\n",
       " 275,\n",
       " 276,\n",
       " 7,\n",
       " 277,\n",
       " 278,\n",
       " 41,\n",
       " 279,\n",
       " 45,\n",
       " 6,\n",
       " 7,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 285,\n",
       " 128,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 149,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 252,\n",
       " 294,\n",
       " 8,\n",
       " 109,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 299,\n",
       " 8,\n",
       " 8,\n",
       " 11,\n",
       " 300,\n",
       " 113,\n",
       " 301,\n",
       " 41,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 292,\n",
       " 306,\n",
       " 105,\n",
       " 307,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 308,\n",
       " 309,\n",
       " 7,\n",
       " 125,\n",
       " 310,\n",
       " 278,\n",
       " 8,\n",
       " 292,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 311,\n",
       " 41,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 109,\n",
       " 254,\n",
       " 29,\n",
       " 325,\n",
       " 34,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 327,\n",
       " 109,\n",
       " 4,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 41,\n",
       " 336,\n",
       " 337,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 7,\n",
       " 268,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 34,\n",
       " 348,\n",
       " 41,\n",
       " 99,\n",
       " 97,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 64,\n",
       " 352,\n",
       " 353,\n",
       " 213,\n",
       " 51,\n",
       " 7,\n",
       " 34,\n",
       " 214,\n",
       " 41,\n",
       " 354,\n",
       " 86,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 6,\n",
       " 15,\n",
       " 141,\n",
       " 360,\n",
       " 278,\n",
       " 361,\n",
       " 41,\n",
       " 362,\n",
       " 216,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 363,\n",
       " 128,\n",
       " 364,\n",
       " 228,\n",
       " 365,\n",
       " 67,\n",
       " 68,\n",
       " 8,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 7,\n",
       " 73,\n",
       " 27,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 371,\n",
       " 128,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 109,\n",
       " 254,\n",
       " 29,\n",
       " 377,\n",
       " 34,\n",
       " 76,\n",
       " 378,\n",
       " 379,\n",
       " 41,\n",
       " 380,\n",
       " 185,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 381,\n",
       " 382,\n",
       " 7,\n",
       " 254,\n",
       " 29,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 99,\n",
       " 100,\n",
       " 214,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 344,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 86,\n",
       " 11,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 7,\n",
       " 34,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 109,\n",
       " 413,\n",
       " 66,\n",
       " 34,\n",
       " 414,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 109,\n",
       " 421,\n",
       " 422,\n",
       " 41,\n",
       " 281,\n",
       " 34,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 128,\n",
       " 428,\n",
       " 429,\n",
       " 7,\n",
       " 76,\n",
       " 430,\n",
       " 431,\n",
       " 105,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 109,\n",
       " 8,\n",
       " 437,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 438,\n",
       " 128,\n",
       " 439,\n",
       " 440,\n",
       " 67,\n",
       " 7,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 15,\n",
       " 446,\n",
       " 27,\n",
       " 447,\n",
       " 448,\n",
       " 7,\n",
       " 8,\n",
       " 254,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 449,\n",
       " 108,\n",
       " 109,\n",
       " 88,\n",
       " 89,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 8,\n",
       " 116,\n",
       " 117,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 450,\n",
       " 8,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 7,\n",
       " 73,\n",
       " 27,\n",
       " 275,\n",
       " 67,\n",
       " 68,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 76,\n",
       " 457,\n",
       " 458,\n",
       " 34,\n",
       " 10,\n",
       " 459,\n",
       " 460,\n",
       " 454,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 67,\n",
       " 468,\n",
       " 469,\n",
       " 113,\n",
       " 174,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 113,\n",
       " 473,\n",
       " 474,\n",
       " 34,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 337,\n",
       " 64,\n",
       " 481,\n",
       " 34,\n",
       " 482,\n",
       " 109,\n",
       " 483,\n",
       " 254,\n",
       " 484,\n",
       " 485,\n",
       " 113,\n",
       " 486,\n",
       " 34,\n",
       " 109,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 490,\n",
       " 491,\n",
       " 492,\n",
       " 493,\n",
       " 494,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 495,\n",
       " 496,\n",
       " 497,\n",
       " 109,\n",
       " 498,\n",
       " 85,\n",
       " 499,\n",
       " 113,\n",
       " 34,\n",
       " 500,\n",
       " 76,\n",
       " 378,\n",
       " 379,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 501,\n",
       " 502,\n",
       " 302,\n",
       " 503,\n",
       " 7,\n",
       " 8,\n",
       " 258,\n",
       " 11,\n",
       " 504,\n",
       " 461,\n",
       " 505,\n",
       " 506,\n",
       " 67,\n",
       " 7,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 511,\n",
       " 512,\n",
       " 34,\n",
       " 51,\n",
       " 290,\n",
       " 291,\n",
       " 513,\n",
       " 7,\n",
       " 514,\n",
       " 109,\n",
       " 4,\n",
       " 46,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 515,\n",
       " 516,\n",
       " 60,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 34,\n",
       " 328,\n",
       " 109,\n",
       " 520,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 517,\n",
       " 524,\n",
       " 434,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 18,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 528,\n",
       " 529,\n",
       " 34,\n",
       " 530,\n",
       " 531,\n",
       " 358,\n",
       " 76,\n",
       " 532,\n",
       " 533,\n",
       " 11,\n",
       " 534,\n",
       " 7,\n",
       " 535,\n",
       " 536,\n",
       " 151,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 537,\n",
       " 128,\n",
       " 538,\n",
       " 539,\n",
       " 76,\n",
       " 540,\n",
       " 13,\n",
       " 541,\n",
       " 113,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 109,\n",
       " 8,\n",
       " 76,\n",
       " 545,\n",
       " 546,\n",
       " 547,\n",
       " 67,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 548,\n",
       " 8,\n",
       " 254,\n",
       " 549,\n",
       " 550,\n",
       " 551,\n",
       " 152,\n",
       " 552,\n",
       " 51,\n",
       " 7,\n",
       " 553,\n",
       " 554,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 555,\n",
       " 11,\n",
       " 556,\n",
       " 557,\n",
       " 558,\n",
       " 7,\n",
       " 559,\n",
       " 560,\n",
       " 561,\n",
       " 562,\n",
       " 109,\n",
       " 563,\n",
       " 322,\n",
       " 34,\n",
       " 564,\n",
       " 11,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 565,\n",
       " 566,\n",
       " 460,\n",
       " 567,\n",
       " 262,\n",
       " 568,\n",
       " 569,\n",
       " 123,\n",
       " 51,\n",
       " 7,\n",
       " 570,\n",
       " 571,\n",
       " 34,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 572,\n",
       " 128,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 576,\n",
       " ...]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words2idxs(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches_generator(batch_size, tokens, tags,\n",
    "                      shuffle=True, allow_smaller_last_batch=True):\n",
    "    \"\"\"Generates padded batches of tokens and tags.\"\"\"\n",
    "    \n",
    "    n_samples = len(tokens)\n",
    "    if shuffle:\n",
    "        order = np.random.permutation(n_samples)\n",
    "    else:\n",
    "        order = np.arange(n_samples)\n",
    "\n",
    "    n_batches = n_samples // batch_size\n",
    "    if allow_smaller_last_batch and n_samples % batch_size:\n",
    "        n_batches += 1\n",
    "\n",
    "    for k in range(n_batches):\n",
    "        batch_start = k * batch_size\n",
    "        batch_end = min((k + 1) * batch_size, n_samples)\n",
    "        current_batch_size = batch_end - batch_start\n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        max_len_token = 0\n",
    "        for idx in order[batch_start: batch_end]:\n",
    "            x_list.append(words2idxs(tokens[idx]))\n",
    "            y_list.append(tags2idxs(tags[idx]))\n",
    "            max_len_token = max(max_len_token, len(tags[idx]))\n",
    "        print(x_list)\n",
    "            \n",
    "        # Fill in the data into numpy nd-arrays filled with padding indices.\n",
    "        x = np.ones([current_batch_size, max_len_token], dtype=np.int32) * token2idx['<PAD>']\n",
    "        y = np.ones([current_batch_size, max_len_token], dtype=np.int32) * tag2idx['O']\n",
    "        lengths = np.zeros(current_batch_size, dtype=np.int32)\n",
    "        for n in range(current_batch_size):\n",
    "            utt_len = len(x_list[n])\n",
    "            print(utt_len)\n",
    "            x[n, :utt_len] = x_list[n]\n",
    "            print(x[n, :utt_len])\n",
    "            lengths[n] = utt_len\n",
    "            print(lengths[n])\n",
    "            y[n, :utt_len] = y_list[n]\n",
    "        yield x, y, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMModel():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Placeholders:\n",
    " - *input_batch* — sequences of words (the shape equals to [batch_size, sequence_len]);\n",
    " - *ground_truth_tags* — sequences of tags (the shape equals to [batch_size, sequence_len]);\n",
    " - *lengths* — lengths of not padded sequences (the shape equals to [batch_size]);\n",
    " - *dropout_ph* — dropout keep probability; this placeholder has a predefined value 1;\n",
    " - *learning_rate_ph* — learning rate; we need this placeholder because we want to change the value during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def declare_placeholders(self):\n",
    "    \"\"\"Specifies placeholders for the model.\"\"\"\n",
    "\n",
    "    # Placeholders for input and ground truth output.\n",
    "    self.input_batch = tf.placeholder(dtype=tf.int32, shape=[None, None], name='input_batch') \n",
    "    self.ground_truth_tags = tf.placeholder(dtype=tf.int32, shape=[None, None], name='ground_truth_tags')\n",
    "  \n",
    "    # Placeholder for lengths of the sequences.\n",
    "    self.lengths = tf.placeholder(dtype=tf.int32, shape=[None], name='lengths')\n",
    "    \n",
    "    # Placeholder for a dropout keep probability. If we don't feed\n",
    "    # a value for this placeholder, it will be equal to 1.0.\n",
    "    self.dropout_ph = tf.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[])\n",
    "    \n",
    "    # Placeholder for a learning rate (tf.float32).\n",
    "    self.learning_rate_ph = tf.placeholder_with_default(1e4, shape=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__declare_placeholders = classmethod(declare_placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_layers(self, vocabulary_size, embedding_dim, n_hidden_rnn, n_tags):\n",
    "    \"\"\"Specifies bi-LSTM architecture and computes logits for inputs.\"\"\"\n",
    "    \n",
    "    # Create embedding variable (tf.Variable) with dtype tf.float32\n",
    "    initial_embedding_matrix = np.random.randn(vocabulary_size, embedding_dim) / np.sqrt(embedding_dim)\n",
    "    embedding_matrix_variable = tf.Variable(initial_embedding_matrix, name='embeddings_matrix', dtype=tf.float32)\n",
    "    \n",
    "    # Create RNN cells (for example, tf.nn.rnn_cell.BasicLSTMCell) with n_hidden_rnn number of units \n",
    "    # and dropout (tf.nn.rnn_cell.DropoutWrapper), initializing all *_keep_prob with dropout placeholder.\n",
    "    forward_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "        tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden_rnn, forget_bias=3.0),\n",
    "        input_keep_prob=self.dropout_ph,\n",
    "        output_keep_prob=self.dropout_ph,\n",
    "        state_keep_prob=self.dropout_ph\n",
    "    )\n",
    "    backward_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "        tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden_rnn, forget_bias=3.0),\n",
    "        input_keep_prob=self.dropout_ph,\n",
    "        output_keep_prob=self.dropout_ph,\n",
    "        state_keep_prob=self.dropout_ph\n",
    "    )\n",
    "\n",
    "    # Look up embeddings for self.input_batch (tf.nn.embedding_lookup).\n",
    "    # Shape: [batch_size, sequence_len, embedding_dim].\n",
    "    embeddings = tf.nn.embedding_lookup(embedding_matrix_variable, self.input_batch)\n",
    "    \n",
    "    # Pass them through Bidirectional Dynamic RNN (tf.nn.bidirectional_dynamic_rnn).\n",
    "    # Shape: [batch_size, sequence_len, 2 * n_hidden_rnn]. \n",
    "    # Also don't forget to initialize sequence_length as self.lengths and dtype as tf.float32.\n",
    "    (rnn_output_fw, rnn_output_bw), _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw= forward_cell, cell_bw= backward_cell,\n",
    "        dtype=tf.float32,\n",
    "        inputs=embeddings,\n",
    "        sequence_length=self.lengths\n",
    "    )\n",
    "    rnn_output = tf.concat([rnn_output_fw, rnn_output_bw], axis=2)\n",
    "\n",
    "    # Dense layer on top.\n",
    "    # Shape: [batch_size, sequence_len, n_tags].   \n",
    "    self.logits = tf.layers.dense(rnn_output, n_tags, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__build_layers = classmethod(build_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_predictions(self):\n",
    "    \"\"\"Transforms logits to probabilities and finds the most probable tags.\"\"\"\n",
    "    \n",
    "    # Create softmax (tf.nn.softmax) function\n",
    "    softmax_output = tf.nn.softmax(self.logits)\n",
    "    \n",
    "    # Use argmax (tf.argmax) to get the most probable tags\n",
    "    # Don't forget to set axis=-1\n",
    "    # otherwise argmax will be calculated in a wrong way\n",
    "    self.predictions = tf.argmax(softmax_output, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__compute_predictions = classmethod(compute_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(self, n_tags, PAD_index):\n",
    "    \"\"\"Computes masked cross-entopy loss with logits.\"\"\"\n",
    "    \n",
    "    # Create cross entropy function function (tf.nn.softmax_cross_entropy_with_logits)\n",
    "    ground_truth_tags_one_hot = tf.one_hot(self.ground_truth_tags, n_tags)\n",
    "    loss_tensor = tf.nn.softmax_cross_entropy_with_logits(labels=ground_truth_tags_one_hot, logits=self.logits)\n",
    "    \n",
    "    # Create loss function which doesn't operate with <PAD> tokens (tf.reduce_mean)\n",
    "    mask = tf.cast(tf.not_equal(loss_tensor, PAD_index), tf.float32)\n",
    "    self.loss =  tf.reduce_mean(tf.reduce_sum(tf.multiply(loss_tensor, mask), axis=-1) / tf.reduce_sum(mask, axis=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__compute_loss = classmethod(compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_optimization(self):\n",
    "    \"\"\"Specifies the optimizer and train_op for the model.\"\"\"\n",
    "    \n",
    "    # Create an optimizer (tf.train.AdamOptimizer)\n",
    "    self.optimizer = tf.train.AdamOptimizer(self.learning_rate_ph)\n",
    "    self.grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
    "    \n",
    "    # Gradient clipping (tf.clip_by_norm) for self.grads_and_vars\n",
    "    # Pay attention that you need to apply this operation only for gradients \n",
    "    # because self.grads_and_vars contains also variables.\n",
    "    # list comprehension might be useful in this case.\n",
    "    clip_norm = tf.cast(1.0, tf.float32)\n",
    "    self.grads_and_vars = [(tf.clip_by_norm(grad, clip_norm), var) for grad, var in self.grads_and_vars]\n",
    "    \n",
    "    self.train_op = self.optimizer.apply_gradients(self.grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__perform_optimization = classmethod(perform_optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(self, vocabulary_size, n_tags, embedding_dim, n_hidden_rnn, PAD_index):\n",
    "    self.__declare_placeholders()\n",
    "    self.__build_layers(vocabulary_size, embedding_dim, n_hidden_rnn, n_tags)\n",
    "    self.__compute_predictions()\n",
    "    self.__compute_loss(n_tags, PAD_index)\n",
    "    self.__perform_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__init__ = classmethod(init_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network and predict tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(self, session, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability):\n",
    "    feed_dict = {self.input_batch: x_batch,\n",
    "                 self.ground_truth_tags: y_batch,\n",
    "                 self.learning_rate_ph: learning_rate,\n",
    "                 self.dropout_ph: dropout_keep_probability,\n",
    "                 self.lengths: lengths}\n",
    "    \n",
    "    session.run(self.train_op, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.train_on_batch = classmethod(train_on_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_batch(self, session, x_batch, lengths):\n",
    "    \n",
    "    predictions = session.run(self.predictions, feed_dict={self.input_batch:x_batch, self.lengths:lengths})\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.predict_for_batch = classmethod(predict_for_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import precision_recall_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tags(model, session, token_idxs_batch, lengths):\n",
    "    \"\"\"Performs predictions and transforms indices to tokens and tags.\"\"\"\n",
    "    \n",
    "    tag_idxs_batch = model.predict_for_batch(session, token_idxs_batch, lengths)\n",
    "    \n",
    "    tags_batch, tokens_batch = [], []\n",
    "    for tag_idxs, token_idxs in zip(tag_idxs_batch, token_idxs_batch):\n",
    "        tags, tokens = [], []\n",
    "        for tag_idx, token_idx in zip(tag_idxs, token_idxs):\n",
    "            tags.append(idx2tag[tag_idx])\n",
    "            tokens.append(idx2token[token_idx])\n",
    "        tags_batch.append(tags)\n",
    "        tokens_batch.append(tokens)\n",
    "    return tags_batch, tokens_batch\n",
    "    \n",
    "    \n",
    "def eval_conll(model, session, tokens, tags, short_report=True):\n",
    "    \"\"\"Computes NER quality measures using CONLL shared task script.\"\"\"\n",
    "    \n",
    "    y_true, y_pred = [], []\n",
    "    for x_batch, y_batch, lengths in batches_generator(1, tokens, tags):\n",
    "        tags_batch, tokens_batch = predict_tags(model, session, x_batch, lengths)\n",
    "        if len(x_batch[0]) != len(tags_batch[0]):\n",
    "            raise Exception(\"Incorrect length of prediction for the input, \"\n",
    "                            \"expected length: %i, got: %i\" % (len(x_batch[0]), len(tags_batch[0])))\n",
    "        predicted_tags = []\n",
    "        ground_truth_tags = []\n",
    "        for gt_tag_idx, pred_tag, token in zip(y_batch[0], tags_batch[0], tokens_batch[0]): \n",
    "            if token != '<PAD>':\n",
    "                ground_truth_tags.append(idx2tag[gt_tag_idx])\n",
    "                predicted_tags.append(pred_tag)\n",
    "\n",
    "        # We extend every prediction and ground truth sequence with 'O' tag\n",
    "        # to indicate a possible end of entity.\n",
    "        y_true.extend(ground_truth_tags + ['O'])\n",
    "        y_pred.extend(predicted_tags + ['O'])\n",
    "        \n",
    "    results = precision_recall_f1(y_true, y_pred, print_results=True, short_report=short_report)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = BiLSTMModel(20505, 21, 200, 200, token2idx['<PAD>'])\n",
    "\n",
    "batch_size = 32\n",
    "n_epochs = 25\n",
    "learning_rate = 0.02\n",
    "learning_rate_decay = 1.4\n",
    "dropout_keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training... \n",
      "\n",
      "-------------------- Epoch 1 of 25 --------------------\n",
      "Train data evaluation:\n",
      "[[213, 403, 18, 386, 213]]\n",
      "5\n",
      "[213 403  18 386 213]\n",
      "5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot copy sequence with size 7 to array axis with dimension 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-581a4105c0d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' Epoch {} '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'of {} '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train data evaluation:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0meval_conll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshort_report\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-170-4ab8fb185d5f>\u001b[0m in \u001b[0;36meval_conll\u001b[0;34m(model, session, tokens, tags, short_report)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mtags_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-150-f2ee440f1c1e>\u001b[0m in \u001b[0;36mbatches_generator\u001b[0;34m(batch_size, tokens, tags, shuffle, allow_smaller_last_batch)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mlengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutt_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mutt_len\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot copy sequence with size 7 to array axis with dimension 5"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print('Start training... \\n')\n",
    "for epoch in range(n_epochs):\n",
    "    # For each epoch evaluate the model on train data\n",
    "    print('-' * 20 + ' Epoch {} '.format(epoch+1) + 'of {} '.format(n_epochs) + '-' * 20)\n",
    "    print('Train data evaluation:')\n",
    "    eval_conll(model, sess, train_tokens, train_tags, short_report=True)\n",
    "    \n",
    "    \n",
    "    # Train the model\n",
    "    for x_batch, y_batch, lengths in batches_generator(batch_size, train_tokens, train_tags):\n",
    "        model.train_on_batch(sess, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability)\n",
    "        \n",
    "    # Decaying the learning rate\n",
    "    learning_rate = learning_rate / learning_rate_decay\n",
    "    \n",
    "print('...training finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-' * 20 + ' Train set quality: ' + '-' * 20)\n",
    "train_results = eval_conll(model, sess, train_tokens, train_tags, short_report=False)\n",
    "\n",
    "print('-' * 20 + ' Test set quality: ' + '-' * 20)\n",
    "test_results = eval_conll(model, sess, test_tokens, test_tags, short_report=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
